"""
EV-7: Report generator.

Produces JSON and Markdown reports from eval runs.
Can generate summary reports, detailed episode breakdowns,
and regression alert digests.
"""

from __future__ import annotations

import json
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from pkg.models.eval import EvalRun, RegressionSeverity


class ReportGenerator:
    """Generates eval reports in multiple formats."""

    def __init__(self, output_dir: str = "reports") -> None:
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def generate_json(self, eval_run: EvalRun) -> str:
        """Write eval run to JSON file. Returns file path."""
        filename = f"eval-{eval_run.run_id[:8]}-{_timestamp()}.json"
        path = self.output_dir / filename

        data = eval_run.model_dump(mode="json")
        path.write_text(json.dumps(data, indent=2, default=str))
        return str(path)

    def generate_markdown(self, eval_run: EvalRun) -> str:
        """Generate a Markdown summary report. Returns file path."""
        filename = f"eval-{eval_run.run_id[:8]}-{_timestamp()}.md"
        path = self.output_dir / filename

        lines = [
            f"# Eval Report: {eval_run.run_id[:8]}",
            "",
            f"**Status:** {eval_run.status.value}",
            f"**Started:** {eval_run.started_at}",
            f"**Ended:** {eval_run.ended_at or 'N/A'}",
            "",
            "## Summary",
            "",
            f"| Metric | Value |",
            f"|--------|-------|",
            f"| Total Episodes | {eval_run.total_episodes} |",
            f"| Passed | {eval_run.passed} |",
            f"| Failed | {eval_run.failed} |",
            f"| Avg Weighted Score | {eval_run.avg_weighted_score:.2f} |",
            f"| Alerts | {len(eval_run.alerts)} |",
            "",
        ]
        # Alerts section
        if eval_run.alerts:
            lines.append("## Regression Alerts")
            lines.append("")
            lines.append("| Severity | Dimension | Episode | Message |")
            lines.append("|----------|-----------|---------|---------|")
            for alert in eval_run.alerts:
                emoji = {"critical": "ðŸ”´", "warning": "ðŸŸ¡", "info": "ðŸ”µ"}.get(
                    alert.severity.value, "âšª"
                )
                lines.append(
                    f"| {emoji} {alert.severity.value} | {alert.dimension.value} "
                    f"| `{alert.episode_id[:8]}` | {alert.message} |"
                )
            lines.append("")

        # Episode details
        if eval_run.results:
            lines.append("## Episode Scores")
            lines.append("")
            lines.append("| Episode | Agent | Score | Correctness | Cost Î” | Latency Î” | Tools | Safety |")
            lines.append("|---------|-------|-------|-------------|--------|-----------|-------|--------|")
            for r in eval_run.results:
                if r.score_card:
                    c = r.score_card
                    lines.append(
                        f"| `{c.episode_id[:8]}` | {c.agent_id} | **{c.weighted_score:.2f}** "
                        f"| {c.correctness:.2f} | {c.cost_delta:+.1f}% | {c.latency_delta:+.1f}% "
                        f"| {c.tool_match:.2f} | {c.safety:.2f} |"
                    )
                else:
                    lines.append(
                        f"| `{r.episode_id[:8]}` | {r.agent_id} | ERROR | - | - | - | - | - |"
                    )

        lines.append("")
        lines.append("---")
        lines.append("*Generated by eval-harness*")

        path.write_text("\n".join(lines))
        return str(path)
    def generate_summary_dict(self, eval_run: EvalRun) -> dict[str, Any]:
        """Return a summary dict (useful for programmatic consumption)."""
        critical_alerts = [a for a in eval_run.alerts if a.severity == RegressionSeverity.CRITICAL]
        warning_alerts = [a for a in eval_run.alerts if a.severity == RegressionSeverity.WARNING]

        return {
            "run_id": eval_run.run_id,
            "status": eval_run.status.value,
            "total_episodes": eval_run.total_episodes,
            "passed": eval_run.passed,
            "failed": eval_run.failed,
            "avg_score": eval_run.avg_weighted_score,
            "critical_alerts": len(critical_alerts),
            "warning_alerts": len(warning_alerts),
            "pass_rate": round(eval_run.passed / max(eval_run.total_episodes, 1), 2),
        }


def _timestamp() -> str:
    """Short timestamp for filenames."""
    return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")